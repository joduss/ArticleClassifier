{"class_name": "Tokenizer", "config": {"num_words": null, "filters": "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n", "lower": true, "split": " ", "char_level": false, "oov_token": null, "document_count": 2, "word_counts": "{\"theme1\": 2, \"theme2\": 1, \"theme3\": 1, \"theme4\": 1}", "word_docs": "{\"theme1\": 2, \"theme2\": 1, \"theme3\": 1, \"theme4\": 1}", "index_docs": "{\"1\": 2, \"2\": 1, \"3\": 1, \"4\": 1}", "index_word": "{\"1\": \"theme1\", \"2\": \"theme2\", \"3\": \"theme3\", \"4\": \"theme4\"}", "word_index": "{\"theme1\": 1, \"theme2\": 2, \"theme3\": 3, \"theme4\": 4}"}}